{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedd30a3",
   "metadata": {},
   "source": [
    "# Graphormer论文笔记（二）\n",
    "> 代码来自于：https://github.com/ytchx1999/Graphormer/blob/main/graphormer/\n",
    "本篇笔记主要分析Graphormer模型中的几段代码，包含主要的transformer的组件，最短距离嵌入bias，节点度的嵌入方法等。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0b0f4f",
   "metadata": {},
   "source": [
    "### 1、主要组件：\n",
    "```python\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, ffn_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer2 = nn.Linear(ffn_size, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "```\n",
    "FeedForwardNetwork类主要是FFN层，了解了transformer的架构该模块不需要单独介绍，只需要注意几点，这里中间的激活函数用的gelu，并且论文中提过这里并没有用传统的升四维再降维的方式而是中间隐藏层维度不变，原论文经过测试这种方式没有明显的性能损失。\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attention_dropout_rate, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.att_size = att_size = hidden_size // num_heads\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.linear_k = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.linear_v = nn.Linear(hidden_size, num_heads * att_size)\n",
    "        self.att_dropout = nn.Dropout(attention_dropout_rate)\n",
    "\n",
    "        self.output_layer = nn.Linear(num_heads * att_size, hidden_size)\n",
    "\n",
    "    def forward(self, q, k, v, attn_bias=None):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.num_heads, d_k)\n",
    "        k = self.linear_k(k).view(batch_size, -1, self.num_heads, d_k)\n",
    "        v = self.linear_v(v).view(batch_size, -1, self.num_heads, d_v)\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q = q * self.scale\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        if attn_bias is not None:\n",
    "            x = x + attn_bias\n",
    "\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.num_heads * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "```\n",
    "这里的MultiHeadAttention层也不需要多介绍，q、k、v的生成是直接使用linear层生成的，这里的多头直接在隐藏层基础上分开头的维度即可，最后还加了一层linear层，防止向下取整后隐藏层维度对不齐的情况。\n",
    "\n",
    "```python\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, ffn_size, dropout_rate, attention_dropout_rate, num_heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size)\n",
    "        self.self_attention = MultiHeadAttention(\n",
    "            hidden_size, attention_dropout_rate, num_heads)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, ffn_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, attn_bias=None):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, attn_bias)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "```\n",
    "这里的EncoderLayer层就是将左右组件构成的Graphormer的编码层，只需要注意16行添加的bias是最短距离bias和edge-feature bias。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67e7759",
   "metadata": {},
   "source": [
    "### 2、Enmbedding层：\n",
    "```python\n",
    "if dataset_name == 'ZINC':\n",
    "    self.atom_encoder = nn.Embedding(64, hidden_dim, padding_idx=0)\n",
    "    self.edge_encoder = nn.Embedding(64, num_heads, padding_idx=0)\n",
    "    self.edge_type = edge_type\n",
    "    if self.edge_type == 'multi_hop':\n",
    "        self.edge_dis_encoder = nn.Embedding(\n",
    "            40 * num_heads * num_heads, 1)\n",
    "    self.rel_pos_encoder = nn.Embedding(40, num_heads, padding_idx=0)\n",
    "    self.in_degree_encoder = nn.Embedding(\n",
    "        64, hidden_dim, padding_idx=0)\n",
    "    self.out_degree_encoder = nn.Embedding(\n",
    "        64, hidden_dim, padding_idx=0)\n",
    "else:\n",
    "    self.atom_encoder = nn.Embedding(\n",
    "        512 * 9 + 1, hidden_dim, padding_idx=0)\n",
    "    self.edge_encoder = nn.Embedding(\n",
    "        512 * 3 + 1, num_heads, padding_idx=0)\n",
    "    self.edge_type = edge_type\n",
    "    if self.edge_type == 'multi_hop':\n",
    "        self.edge_dis_encoder = nn.Embedding(\n",
    "            128 * num_heads * num_heads, 1)\n",
    "    self.rel_pos_encoder = nn.Embedding(512, num_heads, padding_idx=0)\n",
    "    self.in_degree_encoder = nn.Embedding(\n",
    "        512, hidden_dim, padding_idx=0)\n",
    "    self.out_degree_encoder = nn.Embedding(\n",
    "        512, hidden_dim, padding_idx=0)\n",
    "```\n",
    "这里可以看到zinc数据集Embedding层输入维度为64，应该表示字典加上unk最大为64种标号，其他的数据集输入维度原子嵌入为519\\*9个维度，因为ogbg-mol\\*数据的特征维度是9维，边的维度是3维，所以边数据的嵌入维度是512\\*3，由于输入原始特征是多维的（9维或3维）因此还需要做一个编码将维度加到一维再做Embedding代码如下：\n",
    "\n",
    "```python\n",
    "def convert_to_single_emb(x, offset=512):\n",
    "    feature_num = x.size(1) if len(x.size()) > 1 else 1\n",
    "    feature_offset = 1 + \\\n",
    "        torch.arange(0, feature_num * offset, offset, dtype=torch.long)\n",
    "    x = x + feature_offset\n",
    "    return x\n",
    "```\n",
    "\n",
    "另外中心度的bias计算也是使用的Embedding层，但是输出维度不再是隐藏层的特征维度了，是多头的头数，因为这个bias是直接加到attention矩阵中的。最后的到的bias矩阵是[num_of_nodes, num_of_nodes, heads]大小的。以及最短距离bias输入维度也是512，因为最短距离范围只能在0到图中最大节点总数个数之间。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
